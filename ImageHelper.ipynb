{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "import tensorflow as tf\n",
    "import socketio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "from flask import Flask, render_template\n",
    "from io import BytesIO\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from numpy import array\n",
    "import random\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy\n",
    "from PIL import Image\n",
    "\n",
    "def resize_image_to_square(numpy_array_image, new_size):\n",
    "    \"\"\"\n",
    "    I am NOT zero-padding at this moment, \n",
    "    just resizing for the longest size is equal to new_size.\n",
    "    The zero-padding can effectively by done later,\n",
    "    for example during machine learning.\n",
    "    There is no point of wasting space with\n",
    "    thens of thousands padded padded images. \n",
    "    \"\"\"\n",
    "    # convert nympy array image to PIL.Image\n",
    "    image = Image.fromarray(numpy.uint8(numpy_array_image))\n",
    "    old_width = float(image.size[0])\n",
    "    old_height = float(image.size[1])\n",
    "    \n",
    "    if old_width > old_height:\n",
    "        # by width since it is longer\n",
    "        new_width = new_size\n",
    "        ratio = float(new_width / old_width)\n",
    "        new_height = int(old_height * ratio)\n",
    "    else:\n",
    "        # by height since it is longer\n",
    "        new_width = new_size\n",
    "        ratio = float(new_width / old_width)\n",
    "        new_height = int(old_height * ratio)\n",
    "        \n",
    "    image = image.resize((new_width, new_height), PIL.Image.ANTIALIAS)\n",
    "    # turn image into nympy array again\n",
    "    return array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mask_vertices(image):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    top_left = (width*0.3, 0)\n",
    "    top_right = (width-width*0.3, 0)\n",
    "     \n",
    "    mid_left_high = (0, height*0.2) \n",
    "    mid_right_high = (width, height*0.2)  \n",
    "    \n",
    "    mid_left_low = (0, height*0.9) \n",
    "    mid_right_low = (width, height*0.9)\n",
    "    \n",
    "    # on the bottom start high because of the dashboard\n",
    "    bottom_center_left = (width*0.27, height*0.95) \n",
    "    bottom_center_right = (width-width*0.27, height*0.95) \n",
    "    \n",
    "    # we are NOT following a center line in this code, so cut it out\n",
    "    bottom_center = (width/2, height*0.55) \n",
    "\n",
    "\n",
    "    # add points clockwise\n",
    "    vertices = np.array([[\n",
    "        top_left, \n",
    "        top_right, \n",
    "        mid_right_high, mid_right_low,\n",
    "        bottom_center_right,\n",
    "        bottom_center, bottom_center_left,\n",
    "        mid_left_low, \n",
    "        mid_left_high \n",
    "    ]], dtype=np.int32)\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def canny(image, low_threshold=50, high_threshold=250): \n",
    "    # homework low_threshold=20, high_threshold=130\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(image, low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gaussian_blur(image, kernel_size=5): # 5 \n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def grayscale(image):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def crop_image(image):\n",
    "    left = 0\n",
    "    upper = 70\n",
    "    right = 320\n",
    "    lower = 140 # 160 original\n",
    "    image = image.crop((left, upper, right, lower))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_pipline(image, final_size=224, should_plot=False):\n",
    "    \"\"\"\n",
    "    final_size=256 AlexNet and GoogLeNet\n",
    "    final_size=224 VGG-16\n",
    "    final_size=64  is OPTIMAL if I was writing CDNN from scratch\n",
    "    final_size=32  images are fuzzy, AlexNet (street signs CDNN)\n",
    "    final_size=28  images are very fuzzy, LeNet\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if should_plot:\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "    image = array(crop_image(image)) \n",
    "    # convert output to numpy array\n",
    "    if should_plot:\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "    #image = region_of_interest(image, mask_vertices(image))\n",
    "    \n",
    "    image = grayscale(image)\n",
    "    \n",
    "    if should_plot:\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    image = gaussian_blur(image, kernel_size=5)\n",
    "    \n",
    "    if should_plot:\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    image = canny(image, low_threshold=100, high_threshold=190)\n",
    "    \n",
    "    if should_plot:\n",
    "        print(\"image before resizing\", image.shape)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    image = resize_image_to_square(image, new_size=final_size) # VGG-16 size\n",
    "    #image = cv2.resize(image,(224,224))\n",
    "        \n",
    "    if should_plot:\n",
    "        print(\"image after resizing\", image.shape)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.show()\n",
    "    return image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env carnd-term1",
   "language": "python",
   "name": "carnd-term1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
