{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "Please refer to README file for project overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../../DATA/behavioral_cloning_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported rows 8037\n"
     ]
    }
   ],
   "source": [
    "import DataHelper\n",
    "#print(DataHelper.__doc__)\n",
    "from DataHelper import test_read_csv, read_csv\n",
    "#print(read_csv.__doc__)\n",
    "#test_read_csv()\n",
    "# fetch actual log of driving data\n",
    "headers, data = read_csv(data_dir + \"driving_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels (steering value classes)\n",
    "\n",
    "- Please review notebook \"preprocessing\", section: \"Steering value distribution\".\n",
    "- Training labels have values ranging from -1 to +1.\n",
    "- When you steer with **keyboard** the STEPS are rather corse, so I think I can get away with **discrete steering angles, i.e. classes**.\n",
    "- I will start training with 21 equally spread classes, if needed I will increase to 41.\n",
    "- I want to make sure that my classes include **0.0 (zero)** as it is most common value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering_classes [-1.  -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4\n",
      "  0.5  0.6  0.7  0.8  0.9  1. ]\n",
      "Number of classes 21\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuXHxEEcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b1403c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "number_of_classes = 21\n",
    "steering_classes = np.linspace(-1, 1, num=number_of_classes, endpoint=True) \n",
    "steering_classes = np.sort(steering_classes)\n",
    "print(\"steering_classes\", steering_classes)\n",
    "print(\"Number of classes\",steering_classes.shape[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(steering_classes, 'b.')\n",
    "plt.margins(0.1)\n",
    "plt.title(\"Distribution of steering value classes.\")\n",
    "plt.xlabel(\"class number\")\n",
    "plt.ylabel('steering value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Split data into training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent_validation 15\n",
      "training (5626, 7)\n",
      "testing (1206, 7)\n",
      "validation (1204, 7)\n"
     ]
    }
   ],
   "source": [
    "from DataHelper import split_random, get_image_center_values\n",
    "image_center_values = get_image_center_values(data)\n",
    "\n",
    "training, testing, validation = \\\n",
    "split_random(data, percent_train=70, percent_test=15) \n",
    "\n",
    "print(\"training\",training.shape)\n",
    "print(\"testing\",testing.shape)\n",
    "print(\"validation\",validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract training features (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5626,)\n",
      "IMG/center_2016_12_01_13_43_39_086.jpg\n"
     ]
    }
   ],
   "source": [
    "from DataHelper import get_image_center_values \n",
    "image_names = get_image_center_values(training)\n",
    "print(image_names.shape)\n",
    "print(image_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of image paths pointing to 64px version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../DATA/behavioral_cloning_data/processed_images_64/IMG/center_2016_12_01_13_43_39_086.jpg\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "for image_name in image_names:\n",
    "    image_paths.extend([data_dir + \"processed_images_64/\" + image_name])\n",
    "print(image_paths[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape (5626, 14, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABsCAYAAAB6kUkRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e60add8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3  3  3]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ..., \n",
      "  [ 0  0  0]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]]\n",
      "\n",
      " [[ 5  5  5]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ..., \n",
      "  [ 4  4  4]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[33 33 33]\n",
      "  [43 43 43]\n",
      "  [60 60 60]\n",
      "  ..., \n",
      "  [53 53 53]\n",
      "  [46 46 46]\n",
      "  [37 37 37]]\n",
      "\n",
      " ..., \n",
      " [[ 6  6  6]\n",
      "  [ 5  5  5]\n",
      "  [ 3  3  3]\n",
      "  ..., \n",
      "  [65 65 65]\n",
      "  [60 60 60]\n",
      "  [44 44 44]]\n",
      "\n",
      " [[40 40 40]\n",
      "  [40 40 40]\n",
      "  [40 40 40]\n",
      "  ..., \n",
      "  [64 64 64]\n",
      "  [73 73 73]\n",
      "  [68 68 68]]\n",
      "\n",
      " [[17 17 17]\n",
      "  [19 19 19]\n",
      "  [23 23 23]\n",
      "  ..., \n",
      "  [19 19 19]\n",
      "  [17 17 17]\n",
      "  [17 17 17]]]\n"
     ]
    }
   ],
   "source": [
    "from DataHelper import read_image\n",
    "training_features = np.array([ read_image(path) for path in image_paths] )\n",
    "\n",
    "print (\"matrix shape\", training_features.shape)\n",
    "plt.imshow(training_features[2], cmap='gray')\n",
    "plt.show()\n",
    "print (training_features[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from DataHelper import normalize_grayscale\n",
    "\n",
    "training_features_normalized = normalize_grayscale(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract training labels (steering value classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5626,)\n",
      "[ 0.        -0.1452064 -0.0787459  0.         0.       ]\n",
      "['0.0' '-0.1' '-0.1' ..., '0.0' '0.0' '0.0']\n"
     ]
    }
   ],
   "source": [
    "from DataHelper import get_steering_values, find_nearest\n",
    "raw_labels = get_steering_values(training)\n",
    "print(raw_labels.shape)\n",
    "print(raw_labels[0:5])\n",
    "\n",
    "training_labels = np.array([], dtype=np.float64)\n",
    "\n",
    "for raw_label in raw_labels: # raw_labels[:5]\n",
    "    #print(\"raw_label\", raw_label)\n",
    "    label = find_nearest(steering_classes, raw_label)\n",
    "    #print(\"normalized label\", label)\n",
    "    \n",
    "    # Classification labels should be strings:\n",
    "    training_labels = np.append(training_labels, [str(label)])\n",
    "        \n",
    "print(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot = label_binarizer.fit_transform(training_labels)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras (with TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import ELU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation, Lambda\n",
    "\n",
    "from keras.activations import relu, softmax\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "from DataHelper import mean_pred, false_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_features_normalized (5626, 14, 64, 3)\n",
      "y_one_hot (5626, 18)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python' has no attribute 'control_flow_ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a9df3b1b83e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_features_normalized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_features_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_one_hot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Before training a model, you need to configure the learning process, which is done via the compile method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-a9df3b1b83e7>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(weights_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborder_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mELU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ukilucas/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    305\u001b[0m                  output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 raise Exception('All layers in a Sequential model '\n",
      "\u001b[0;32m/Users/ukilucas/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;31m# this will call layer.build() if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_inbound_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0minput_added\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ukilucas/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36madd_inbound_node\u001b[0;34m(self, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# creating the node automatically updates self.inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# as well as outbound_nodes on inbound layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ukilucas/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mcreate_node\u001b[0;34m(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0moutput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# TODO: try to auto-infer shape if exception is raised by get_output_shape_for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ukilucas/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_train_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ukilucas/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36min_train_phase\u001b[0;34m(x, alt)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;31m# else: assume learning phase is a placeholder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m     x = tf.python.control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n\u001b[0m\u001b[1;32m   1108\u001b[0m                                         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         lambda: alt)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python' has no attribute 'control_flow_ops'"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, Convolution1D\n",
    "    \n",
    "def build_model(weights_path=None):\n",
    "    \"\"\"\n",
    "    The cropped camera images are expected to be (3, 14, 64), not square\n",
    "    e.g.: \n",
    "    - training_features_normalized (5626, 14, 64, 3)\n",
    "    - y_one_hot (5626, 18)\n",
    "    \n",
    "    https://keras.io/layers/convolutional/\n",
    "    \"\"\"\n",
    "    output_dim = 21 # number of classes\n",
    "    model = Sequential()\n",
    "    \n",
    "    # IN: (samples, rows, cols, channels)'tf'\n",
    "    # expected 01_Conv2D to have 4 dimensions, but got array with shape (5626, 18)\n",
    "    # expected 01_Conv2D to have 4 dimensions, but got array with shape (5626, 1)\n",
    "    # model.add(Convolution2D(96, 14, 64, dim_ordering='tf', input_shape=(14, 64, 3), activation='relu', name=\"01_Conv2D\"))\n",
    "    # Output shape of convolution is 4d\n",
    "    \n",
    "    #expected flatten_2 to have shape (None, 96) but got array with shape (5626, 18)\n",
    "    #model.add(Flatten()) \n",
    "    # Flatten input into 2d\n",
    "\n",
    "    # Dense layer require a 2d input\n",
    "    # have shape (None, 21) but got array with shape (5626, 1)\n",
    "    #model.add(Dense(18, activation='sigmoid', name=\"001_Dense\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"training_features_normalized\", training_features_normalized.shape)\n",
    "print(\"y_one_hot\", y_one_hot.shape)\n",
    "model = build_model()\n",
    "\n",
    "# Before training a model, you need to configure the learning process, which is done via the compile method.\n",
    "optimizer='sgd' # | 'rmsprop'\n",
    "loss_function='mean_squared_error' # | 'binary_crossentropy' | 'mse'\n",
    "metrics_array=['accuracy', mean_pred, false_rates]\n",
    "model.compile(optimizer, loss_function, metrics_array)\n",
    "\n",
    "history = model.fit(training_features_normalized, training_labels, nb_epoch=3, verbose=1,validation_split=0.2)\n",
    "#history = model.fit(training_features_normalized, y_one_hot, nb_epoch=3, validation_split=0.2)\n",
    "\n",
    "#Epoch 20/20 loss: 0.0518 - acc: 0.60 - val_loss: 0.05 - val_acc: 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
